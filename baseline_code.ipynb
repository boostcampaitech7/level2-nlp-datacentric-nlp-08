{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZFRz66WGJ0m"
   },
   "source": [
    "# Data-Centric NLP 대회: 주제 분류 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ-n74gNGJ0n"
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ieJqZz6WGJ0n"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9MrGeVLGJ0o"
   },
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Rojb26TRGJ0o"
   },
   "outputs": [],
   "source": [
    "SEED = 456\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHiKw7tAGJ0o",
    "outputId": "fa675ab9-3221-4ef1-dabb-42e2aad2192c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANUH4JCxGJ0o"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'resources/processed/v7')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, './resources/output_v7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuP9IW9mGJ0o"
   },
   "source": [
    "## Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HH0lhDvhGJ0o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x2NvoGbGJ0o"
   },
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gE13nELlGJ0o"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('resources/output_v7/train_20241107_1.csv'))\n",
    "# dataset_train, dataset_valid = train_test_split(data, test_size=0.1, random_state=SEED)\n",
    "# print(\"Train 데이터:\")\n",
    "# print(len(dataset_train))\n",
    "# print(\"\\nValid 데이터:\")\n",
    "# print(len(dataset_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[data['source'] == 'noisy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정부, '주파수 미사용' KT에 이용기간 2년 단축 처분(종합2보)</td>\n",
       "      <td>4</td>\n",
       "      <td>IT과학</td>\n",
       "      <td>0.20599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정부, KT 주파수 미사용에 강력한 제재 조치</td>\n",
       "      <td>4</td>\n",
       "      <td>IT과학</td>\n",
       "      <td>0.23120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>KT, 주파수 미사용으로 이용기간 2년 단축 처분</td>\n",
       "      <td>4</td>\n",
       "      <td>IT과학</td>\n",
       "      <td>0.21202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>KT, 주파수 미사용으로 인한 사용 기간의 2년 단축.</td>\n",
       "      <td>4</td>\n",
       "      <td>IT과학</td>\n",
       "      <td>0.22293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>미사용 주파수에 대한 정부와 KT에 대한 강력한 제재.</td>\n",
       "      <td>4</td>\n",
       "      <td>IT과학</td>\n",
       "      <td>0.23634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>찰 국 로 한 회장 송</td>\n",
       "      <td>3</td>\n",
       "      <td>사회</td>\n",
       "      <td>0.18184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>경찰이 한국상공회의소 회장을 비롯해 20여 명의 '국회 불법 로비' 혐의로 구속했다.</td>\n",
       "      <td>3</td>\n",
       "      <td>사회</td>\n",
       "      <td>0.18318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>경찰, '국회 불법 로비' 한어총 회장 등 20명 송치</td>\n",
       "      <td>3</td>\n",
       "      <td>사회</td>\n",
       "      <td>0.17293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>KTU 교원들이 한번 맞으면 탈락 체제 도입에 반대한다.</td>\n",
       "      <td>3</td>\n",
       "      <td>사회</td>\n",
       "      <td>0.22409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>교육계에서 급식용 천막 설치에 대한 논란이 다시 불타오르고 있다.</td>\n",
       "      <td>3</td>\n",
       "      <td>사회</td>\n",
       "      <td>0.22560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                                             text  \\\n",
       "0  ynat-v1_train_00000            정부, '주파수 미사용' KT에 이용기간 2년 단축 처분(종합2보)   \n",
       "1  ynat-v1_train_00000                        정부, KT 주파수 미사용에 강력한 제재 조치   \n",
       "2  ynat-v1_train_00000                      KT, 주파수 미사용으로 이용기간 2년 단축 처분   \n",
       "3  ynat-v1_train_00000                   KT, 주파수 미사용으로 인한 사용 기간의 2년 단축.   \n",
       "4  ynat-v1_train_00000                   미사용 주파수에 대한 정부와 KT에 대한 강력한 제재.   \n",
       "5  ynat-v1_train_00001                                     찰 국 로 한 회장 송   \n",
       "6  ynat-v1_train_00001  경찰이 한국상공회의소 회장을 비롯해 20여 명의 '국회 불법 로비' 혐의로 구속했다.   \n",
       "7  ynat-v1_train_00001                   경찰, '국회 불법 로비' 한어총 회장 등 20명 송치   \n",
       "8  ynat-v1_train_00001                  KTU 교원들이 한번 맞으면 탈락 체제 도입에 반대한다.   \n",
       "9  ynat-v1_train_00001             교육계에서 급식용 천막 설치에 대한 논란이 다시 불타오르고 있다.   \n",
       "\n",
       "   target target_name  similarity  \n",
       "0       4        IT과학     0.20599  \n",
       "1       4        IT과학     0.23120  \n",
       "2       4        IT과학     0.21202  \n",
       "3       4        IT과학     0.22293  \n",
       "4       4        IT과학     0.23634  \n",
       "5       3          사회     0.18184  \n",
       "6       3          사회     0.18318  \n",
       "7       3          사회     0.17293  \n",
       "8       3          사회     0.22409  \n",
       "9       3          사회     0.22560  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터:\n",
      "14010\n",
      "\n",
      "Valid 데이터:\n",
      "1553\n"
     ]
    }
   ],
   "source": [
    "dataset_train = pd.DataFrame()\n",
    "dataset_valid = pd.DataFrame()\n",
    "\n",
    "for target, group in data.groupby('target'):\n",
    "    n = len(group)\n",
    "    valid_size = max(int(n * 0.1), 1)  # 최소 1개는 valid로\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    valid_indices = np.random.choice(group.index, size=valid_size, replace=False)\n",
    "    train_indices = group.index.difference(valid_indices)\n",
    "    \n",
    "    dataset_train = pd.concat([dataset_train, group.loc[train_indices]])\n",
    "    dataset_valid = pd.concat([dataset_valid, group.loc[valid_indices]])\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Train 데이터:\")\n",
    "print(len(dataset_train))\n",
    "print(\"\\nValid 데이터:\")\n",
    "print(len(dataset_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ynat-v1_train_00018</td>\n",
       "      <td>정연희, 본사 작가.</td>\n",
       "      <td>0</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>0.19117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ynat-v1_train_00018</td>\n",
       "      <td>개전 연정의 새 시대, H 작가의 문학적 통찰로 재조명</td>\n",
       "      <td>0</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>0.22476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>ynat-v1_train_00024</td>\n",
       "      <td>서울에 다시 오존주의보 도심 서북 동북권 발령종합</td>\n",
       "      <td>0</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>0.19536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ynat-v1_train_00036</td>\n",
       "      <td>크루즈 관광객용 반나절 부산 해안 트레킹 상품 개발</td>\n",
       "      <td>0</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>0.20650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>ynat-v1_train_00071</td>\n",
       "      <td>영상 냄새가 나는데 퇴근길 마주친 수상한 차 따라가봤더니</td>\n",
       "      <td>0</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>0.20018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ID                             text  target target_name  \\\n",
       "48   ynat-v1_train_00018                      정연희, 본사 작가.       0        생활문화   \n",
       "49   ynat-v1_train_00018   개전 연정의 새 시대, H 작가의 문학적 통찰로 재조명       0        생활문화   \n",
       "62   ynat-v1_train_00024      서울에 다시 오존주의보 도심 서북 동북권 발령종합       0        생활문화   \n",
       "97   ynat-v1_train_00036     크루즈 관광객용 반나절 부산 해안 트레킹 상품 개발       0        생활문화   \n",
       "170  ynat-v1_train_00071  영상 냄새가 나는데 퇴근길 마주친 수상한 차 따라가봤더니       0        생활문화   \n",
       "\n",
       "     similarity  \n",
       "48      0.19117  \n",
       "49      0.22476  \n",
       "62      0.19536  \n",
       "97      0.20650  \n",
       "170     0.20018  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13027</th>\n",
       "      <td>ynat-v1_train_02170</td>\n",
       "      <td>일왕·왕비, 즉위식 거행</td>\n",
       "      <td>0</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>0.19169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>ynat-v1_train_01010</td>\n",
       "      <td>노 김 피 손민 겨 는 베 트 연 곡</td>\n",
       "      <td>0</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>0.19184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13168</th>\n",
       "      <td>ynat-v1_train_00600</td>\n",
       "      <td>도자 예술의 회고적인 전시인 YGLU 전시회에서 만나게 된 혁신적인 작품들.</td>\n",
       "      <td>0</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>0.20621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13945</th>\n",
       "      <td>ynat-v1_train_01034</td>\n",
       "      <td>남쪽 먼 제주 동부 풍 강풍경 대</td>\n",
       "      <td>0</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>0.21113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12399</th>\n",
       "      <td>905</td>\n",
       "      <td>만화박물관 3·1운동 기념벽화 걸고 영화 '동주' 무료 특별 관람 상영</td>\n",
       "      <td>0</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>0.20029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ID                                        text  \\\n",
       "13027  ynat-v1_train_02170                               일왕·왕비, 즉위식 거행   \n",
       "2410   ynat-v1_train_01010                        노 김 피 손민 겨 는 베 트 연 곡   \n",
       "13168  ynat-v1_train_00600  도자 예술의 회고적인 전시인 YGLU 전시회에서 만나게 된 혁신적인 작품들.   \n",
       "13945  ynat-v1_train_01034                          남쪽 먼 제주 동부 풍 강풍경 대   \n",
       "12399                  905     만화박물관 3·1운동 기념벽화 걸고 영화 '동주' 무료 특별 관람 상영   \n",
       "\n",
       "       target target_name  similarity  \n",
       "13027       0        생활문화     0.19169  \n",
       "2410        0        생활문화     0.19184  \n",
       "13168       0        생활문화     0.20621  \n",
       "13945       0        생활문화     0.21113  \n",
       "12399       0        생활문화     0.20029  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "4    2348\n",
       "6    2175\n",
       "0    2021\n",
       "2    1964\n",
       "3    1893\n",
       "1    1881\n",
       "5    1728\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "4    260\n",
       "6    241\n",
       "0    224\n",
       "2    218\n",
       "3    210\n",
       "1    208\n",
       "5    192\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_valid['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9BQVS286GJ0o"
   },
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        input_texts = data['text']\n",
    "        targets = data['target']\n",
    "        self.inputs = []; self.labels = []\n",
    "        for text, label in zip(input_texts, targets):\n",
    "            tokenized_input = tokenizer(text, padding='max_length', truncation=True, return_tensors='pt')\n",
    "            self.inputs.append(tokenized_input)\n",
    "            self.labels.append(torch.tensor(label))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs[idx]['input_ids'].squeeze(0),\n",
    "            'attention_mask': self.inputs[idx]['attention_mask'].squeeze(0),\n",
    "            'labels': self.labels[idx].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BVycj2wPGJ0p"
   },
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, tokenizer)\n",
    "data_valid = BERTDataset(dataset_valid, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5yh5dYa0GJ0p"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPl6TZ7CGJ0p"
   },
   "source": [
    "## Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XA9g2vV_GJ0p"
   },
   "outputs": [],
   "source": [
    "f1 = evaluate.load('f1')\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return f1.compute(predictions=predictions, references=labels, average='macro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WV2exsooGJ0p"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OkQVpzabGJ0p"
   },
   "outputs": [],
   "source": [
    "## for wandb setting\n",
    "os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SLV_Qq5bGJ0p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    logging_strategy='steps',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    # learning_rate= 5e-05,\n",
    "    learning_rate= 3e-05,\n",
    "    # learning_rate= 1e-04,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='linear',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eGAepHgxGJ0p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_valid,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1394' max='1394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1394/1394 13:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.106400</td>\n",
       "      <td>0.581233</td>\n",
       "      <td>0.818006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.495200</td>\n",
       "      <td>0.488878</td>\n",
       "      <td>0.856991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.507613</td>\n",
       "      <td>0.855034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.472658</td>\n",
       "      <td>0.853725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.467572</td>\n",
       "      <td>0.867872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.455710</td>\n",
       "      <td>0.868934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.371400</td>\n",
       "      <td>0.411424</td>\n",
       "      <td>0.886336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.426571</td>\n",
       "      <td>0.880624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.422573</td>\n",
       "      <td>0.886753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.245900</td>\n",
       "      <td>0.393742</td>\n",
       "      <td>0.897721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.410792</td>\n",
       "      <td>0.896899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.387615</td>\n",
       "      <td>0.898234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.386761</td>\n",
       "      <td>0.900506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1394, training_loss=0.3644647919806723, metrics={'train_runtime': 813.5329, 'train_samples_per_second': 27.416, 'train_steps_per_second': 1.714, 'total_flos': 5868692430028800.0, 'train_loss': 0.3644647919806723, 'epoch': 2.0})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f1\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/ksh/level2-nlp-datacentric-nlp-08/.venv_sh/lib/python3.10/site-packages/transformers/trainer.py:3347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "/data/ephemeral/home/ksh/level2-nlp-datacentric-nlp-08/.venv_sh/lib/python3.10/site-packages/transformers/trainer.py:3026: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1752' max='1752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1752/1752 01:25, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.151500</td>\n",
       "      <td>0.266350</td>\n",
       "      <td>0.933515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1752, training_loss=0.012891080553673174, metrics={'train_runtime': 86.0697, 'train_samples_per_second': 325.55, 'train_steps_per_second': 20.356, 'total_flos': 7372702738944000.0, 'train_loss': 0.012891080553673174, 'epoch': 2.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBXeP6ynGJ0p"
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "eifEFgIOGJ0p"
   },
   "outputs": [],
   "source": [
    "dataset_test = pd.read_csv(\"resources/raw_data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "7vPFu9y1GJ0p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 30000/30000 [03:46<00:00, 132.61it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "probs = []\n",
    "\n",
    "for idx, sample in tqdm(dataset_test.iterrows(), total=len(dataset_test), desc=\"Evaluating\"):\n",
    "    inputs = tokenizer(sample['text'], return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        prob = torch.nn.Softmax(dim=1)(logits)\n",
    "        pred = torch.argmax(torch.nn.Softmax(dim=1)(logits), dim=1).cpu().numpy()\n",
    "        preds.extend(pred)\n",
    "        probs.extend(prob.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "UO2hzsl-GJ0p"
   },
   "outputs": [],
   "source": [
    "dataset_test['target'] = preds\n",
    "dataset_test.to_csv(os.path.join(BASE_DIR, 'resources/output/output_3.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oO5h88_3GJ0q"
   },
   "outputs": [],
   "source": [
    "# output_with_prob = dataset_test.copy()\n",
    "# probs = np.array(probs)\n",
    "# for i in range(probs.shape[1]):\n",
    "#     output_with_prob[f'prob_{i}'] = probs[:, i]\n",
    "    \n",
    "# output_with_prob.to_csv(os.path.join(BASE_DIR, 'resources/output/output_prob.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv_sh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
