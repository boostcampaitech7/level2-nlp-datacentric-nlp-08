{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZFRz66WGJ0m"
   },
   "source": [
    "# Data-Centric NLP 대회: 주제 분류 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ-n74gNGJ0n"
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ieJqZz6WGJ0n"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9MrGeVLGJ0o"
   },
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Rojb26TRGJ0o"
   },
   "outputs": [],
   "source": [
    "SEED = 456\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHiKw7tAGJ0o",
    "outputId": "fa675ab9-3221-4ef1-dabb-42e2aad2192c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ANUH4JCxGJ0o"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'resources/pre_processed_data')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, '../output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuP9IW9mGJ0o"
   },
   "source": [
    "## Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HH0lhDvhGJ0o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x2NvoGbGJ0o"
   },
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gE13nELlGJ0o"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(DATA_DIR,'train_20241104.csv'))\n",
    "# dataset_train, dataset_valid = train_test_split(data, test_size=0.1, random_state=SEED)\n",
    "# print(\"Train 데이터:\")\n",
    "# print(len(dataset_train))\n",
    "# print(\"\\nValid 데이터:\")\n",
    "# print(len(dataset_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터:\n",
      "10456\n",
      "\n",
      "Valid 데이터:\n",
      "1158\n"
     ]
    }
   ],
   "source": [
    "dataset_train = pd.DataFrame()\n",
    "dataset_valid = pd.DataFrame()\n",
    "\n",
    "for target, group in data.groupby('target'):\n",
    "    n = len(group)\n",
    "    valid_size = max(int(n * 0.1), 1)  # 최소 1개는 valid로\n",
    "    \n",
    "    valid_indices = np.random.choice(group.index, size=valid_size, replace=False)\n",
    "    train_indices = group.index.difference(valid_indices)\n",
    "    \n",
    "    dataset_train = pd.concat([dataset_train, group.loc[train_indices]])\n",
    "    dataset_valid = pd.concat([dataset_valid, group.loc[valid_indices]])\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Train 데이터:\")\n",
    "print(len(dataset_train))\n",
    "print(\"\\nValid 데이터:\")\n",
    "print(len(dataset_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "6    1554\n",
       "1    1553\n",
       "2    1535\n",
       "0    1494\n",
       "3    1463\n",
       "4    1454\n",
       "5    1403\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    172\n",
       "6    172\n",
       "2    170\n",
       "0    166\n",
       "3    162\n",
       "4    161\n",
       "5    155\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_valid['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9BQVS286GJ0o"
   },
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        input_texts = data['text']\n",
    "        targets = data['target']\n",
    "        self.inputs = []; self.labels = []\n",
    "        for text, label in zip(input_texts, targets):\n",
    "            tokenized_input = tokenizer(text, padding='max_length', truncation=True, return_tensors='pt')\n",
    "            self.inputs.append(tokenized_input)\n",
    "            self.labels.append(torch.tensor(label))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs[idx]['input_ids'].squeeze(0),\n",
    "            'attention_mask': self.inputs[idx]['attention_mask'].squeeze(0),\n",
    "            'labels': self.labels[idx].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BVycj2wPGJ0p"
   },
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, tokenizer)\n",
    "data_valid = BERTDataset(dataset_valid, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5yh5dYa0GJ0p"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPl6TZ7CGJ0p"
   },
   "source": [
    "## Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XA9g2vV_GJ0p"
   },
   "outputs": [],
   "source": [
    "f1 = evaluate.load('f1')\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return f1.compute(predictions=predictions, references=labels, average='macro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WV2exsooGJ0p"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OkQVpzabGJ0p"
   },
   "outputs": [],
   "source": [
    "## for wandb setting\n",
    "os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SLV_Qq5bGJ0p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    logging_strategy='steps',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    learning_rate= 2e-05,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='linear',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eGAepHgxGJ0p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_valid,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vJ_Vzpc9GJ0p"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='654' max='654' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [654/654 10:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.240000</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>0.755542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.835700</td>\n",
       "      <td>0.683899</td>\n",
       "      <td>0.794302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.716700</td>\n",
       "      <td>0.628422</td>\n",
       "      <td>0.806925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>0.601929</td>\n",
       "      <td>0.824906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.538200</td>\n",
       "      <td>0.581376</td>\n",
       "      <td>0.827887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.509800</td>\n",
       "      <td>0.562797</td>\n",
       "      <td>0.828998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=654, training_loss=0.7234333516625454, metrics={'train_runtime': 626.4333, 'train_samples_per_second': 33.383, 'train_steps_per_second': 1.044, 'total_flos': 5502425398886400.0, 'train_loss': 0.7234333516625454, 'epoch': 2.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Valid Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        ID                                  text  target  \\\n",
      "65     ynat-v1_train_00018                           개 전 연 정연 작가       0   \n",
      "66     ynat-v1_train_00018        전쟁과 평화의 경계에서, H 작가의 작품이 던지는 질문       0   \n",
      "67     ynat-v1_train_00018  작가 H의 문학적 통찰력으로 재조명된 전전쟁 연합의 새로운 시대.       0   \n",
      "68     ynat-v1_train_00018     전쟁과 평화의 경계에서, H 작가들의 작품에서 제기된 질문들       0   \n",
      "69     ynat-v1_train_00018                           정연희, 본사 작가.       0   \n",
      "...                    ...                                   ...     ...   \n",
      "11576  ynat-v1_train_02787       13일 노바 라이프미주 3개 품목 10% 할인 상품 출시       6   \n",
      "11577  ynat-v1_train_02787                     이 노바 라이 미 어패 결 상품       6   \n",
      "11593  ynat-v1_train_02791          거리에서 노래하던 베네수엘라 이민자에게 찾아온 기적       6   \n",
      "11601  ynat-v1_train_02793         경찰 월초 유커와 日관광객위해 바가지 요금 집중 단속       6   \n",
      "11603  ynat-v1_train_02795         트럼프 폭스뉴스 앵커들 충성도 점수매겨 점만점에 점도       6   \n",
      "\n",
      "      target_name     source  \n",
      "65           생활문화      noisy  \n",
      "66           생활문화      noisy  \n",
      "67           생활문화      noisy  \n",
      "68           생활문화      noisy  \n",
      "69           생활문화      noisy  \n",
      "...           ...        ...  \n",
      "11576          세계      noisy  \n",
      "11577          세계      noisy  \n",
      "11593          세계  not_noisy  \n",
      "11601          세계  not_noisy  \n",
      "11603          세계  not_noisy  \n",
      "\n",
      "[10456 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train)\n",
    "# print(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       ID                                            text  \\\n",
      "1205  ynat-v1_train_00287            프베이스볼 스타 윤호영이 무릎 수술을 위해 650만원을 지원한다.   \n",
      "6877  ynat-v1_train_01617                            화천 산천어 축제 제2회 즐거운 눈썰   \n",
      "3914  ynat-v1_train_00918                  미스터 션샤인, 새로운 시즌 예고...주요 인물들 복귀   \n",
      "1959  ynat-v1_train_00494  대형 서점의 베스트셀러 코너에 있는 책의 다양성 문제를 제기하는 독자들의 불만이다.   \n",
      "4722  ynat-v1_train_01088      메이샨은 일본 회사가 주최한 컨퍼런스에서 뛰어난 작곡으로 우수상을 수상했다.   \n",
      "...                   ...                                             ...   \n",
      "5516  ynat-v1_train_01271                              밀라노 지오디 광장의 야외 광고.   \n",
      "3268  ynat-v1_train_00783                 북미, 치열한 경제 경쟁 속 아E존의 전략적 중요성 부각   \n",
      "5831  ynat-v1_train_01346          초등학교 3부, 혁신적인 Y 교육 방식으로 학생들의 학습 효과 극대화   \n",
      "9390  ynat-v1_train_02268           UOz 감독의 충격적 발언, 영화 '펄프 픽션' 캐릭터 비유로 논란   \n",
      "7285  ynat-v1_train_01734                   트럼프의 루마니아 방문... 정경두 국방장관의 기도.   \n",
      "\n",
      "      target target_name source  \n",
      "1205       0        생활문화  noisy  \n",
      "6877       0        생활문화  noisy  \n",
      "3914       0        생활문화  noisy  \n",
      "1959       0        생활문화  noisy  \n",
      "4722       0        생활문화  noisy  \n",
      "...      ...         ...    ...  \n",
      "5516       6          세계  noisy  \n",
      "3268       6          세계  noisy  \n",
      "5831       6          세계  noisy  \n",
      "9390       6          세계  noisy  \n",
      "7285       6          세계  noisy  \n",
      "\n",
      "[1158 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluation (dataset_test):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    probs = []\n",
    "    for idx, sample in tqdm(dataset_test.iterrows(), total=len(dataset_test), desc=\"Evaluating\"):\n",
    "        inputs = tokenizer(sample['text'], padding='max_length',truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            prob = torch.nn.Softmax(dim=1)(logits)\n",
    "            pred = torch.argmax(torch.nn.Softmax(dim=1)(logits), dim=1).cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            probs.extend(prob.cpu().numpy())\n",
    "    dataset_test['predicted_value'] = preds\n",
    "    dataset_test.to_csv(os.path.join(BASE_DIR, 'resources/analyze/valid_output.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10456/10456 [02:05<00:00, 83.54it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluation(dataset_test=dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1158/1158 [00:13<00:00, 83.48it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluation(dataset_test=dataset_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBXeP6ynGJ0p"
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eifEFgIOGJ0p"
   },
   "outputs": [],
   "source": [
    "dataset_test = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7vPFu9y1GJ0p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  82%|████████▏ | 24697/30000 [03:08<00:40, 131.56it/s]"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "probs = []\n",
    "\n",
    "for idx, sample in tqdm(dataset_test.iterrows(), total=len(dataset_test), desc=\"Evaluating\"):\n",
    "    inputs = tokenizer(sample['text'], return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        prob = torch.nn.Softmax(dim=1)(logits)\n",
    "        pred = torch.argmax(torch.nn.Softmax(dim=1)(logits), dim=1).cpu().numpy()\n",
    "        preds.extend(pred)\n",
    "        probs.extend(prob.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UO2hzsl-GJ0p"
   },
   "outputs": [],
   "source": [
    "dataset_test['target'] = preds\n",
    "dataset_test.to_csv(os.path.join(BASE_DIR, 'resources/output/output.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
